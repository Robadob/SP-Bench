\section{\glsentrylong{hpc}}
  With knowledge that complex simulations demand higher performance and the existing techniques used for handling spatial data, it becomes necessary to discuss the hardware methods used to achieve truly high performance.
  
  Due to the thermodynamic limitations which have largely stalled increases to the speed of processors, it has become necessary to utilise parallel forms of computation to improve the throughput of algorithms which demand high performance. Parallel algorithms can be decomposed by a programmer in one of two styles: task-parallel or data-parallel. Task-parallelism decomposes a problem into threads which operate independently, often on distinct tasks, communicating when necessary. Comparatively data-parallelism decomposes a problem according to the data-set being operated on, such that each thread performs the same algorithm on a different block of data. Task-parallelism is most often found in the form of multi-core \glspl{cpu} and computing clusters. Data-parallelism is most often associated with \gls{gpgpu} computation due to the dedicated architecture found in \glspl{gpu}.
    
  The following subsections discuss some of the techniques used to provide \gls{hpc}. Greater focus is applied to \gls{gpgpu} acceleration as it follows a data-parallel decomposition as opposed to the more familiar task-parallel decomposition.
  
  \subsection{\glsentrylong{gpgpu}\label{sec:gpgpu}}
    \gls{gpgpu} has been greatly popularised in the past decade, moving from programmers re-purposing the rendering pipeline to dedicated languages, frameworks and specialised hardware options. This has led to the power of \gls{gpu} computation being leveraged to speed up processing within many industries. The more energy efficient architecture of \glspl{gpu} has made \gls{gpgpu} a more affordable alternative to the more traditional \gls{hpc} offerings \cite{OH*08}. This isn't to say super computers don't use \gls{gpu} computation, simply that a single \gls{gpu} can often provide significant speed-up to otherwise \gls{cpu} limited code for a relatively conservative price. The performance increase from multi-core \gls{cpu} to \gls{gpu} varies widely between algorithms. Some research suggests 100-fold increases can be achieved, whereas Intel's own research insists if code is correctly optimised for multi-core \glspl{cpu}, this falls to a maximum of 5-fold benefit. There is certainly potential for the lesser awareness of and greater difficulty in applying \gls{cpu} optimisation techniques, having an effect on the divide \cite{LK*10}.

    It is possible to leverage multiple \glspl{gpu} simultaneously to further increase performance. NVIDIA \glspl{gpu} have \gls{sli} functionality which allows 2-4 \glspl{gpu} to work in parallel. In this state each device's memory is mirrored (similar to RAID 1) but their processors work individually. Essentially multiplying the processing power. AMD \glspl{gpu} have a similar Crossfire functionality. \glspl{gpu} can be used together without the use of \gls{sli} or Crossfire, however this requires the programmer to pass data between devices. This can be achieved using \gls{mpi}, a standardised protocol for message passing during parallel computation \cite{MPI}. There are many implementations of \gls{mpi} available, and they can be used from passing messages between individual \glspl{gpu} in the same machine, to passing messages between many machines over the internet.
    
    \glspl{gpu} do not have direct access to the memory of the host system. Transferring data between the host system and a \gls{gpu} occurs over the PCI Express bus and incurs latency and throughput limitations. Volkov and Demmel found these limitations to be a latency of $15\mu s$ and throughput of $3.3 GB/s$ \cite{VD08}, however their research targeted \gls{pcie} 1.1 hardware, whereas current \glspl{gpu} use \gls{pcie} 3.0 which has greatly improved throughput. This latency is 150 times the documented latency of \gls{cpu} memory accesses \cite{INTEL_BENCH}, therefore it is necessary to ensure that enough processing is occurring on the \gls{gpu} to offset any costs incurred by this bottleneck. NVIDIA has however announced a new technology NVLink, which will enable data to be transfered at throughputs 5-12 times faster than those offered by \gls{pcie}\ \cite{NVLINK}.
    
    The following subsections provide an understanding of the underlying architecture found in \gls{gpu} hardware, frameworks used to utilise this hardware and a case-study of the performance gains achieved with \gls{gpgpu} computation on several algorithms.

    \begin{wrapfigure}{O}{0.5\columnwidth}%
    \begin{centering}
      \includegraphics[width=0.5\columnwidth,keepaspectratio]{\string"../resources/warp-execution\string".pdf}
    \par\end{centering}
    \protect\caption[Illustration of the \glsentrylong{simt} parallel execution model used by \acrshortpl{gpu}.]{\label{fig:SIMT}Illustration of the \gls{simt} parallel execution model used by \glspl{gpu}. The threads within the first warp (threads 0-31) contain multiple values of $x$, therefore both branches must be executed separately. All threads within the second warp (threads 32-63) have the value $false$ for $x$, so only the green branch is executed.}
  \end{wrapfigure}%
  
    \subsubsection{\glsentryshort{gpu} Architecture\label{sub:NVIDIA-GPU-Architecture}}
      \emph{In a broad sense each \gls{gpu} consists of several \glspl{sm} and multiple GBs of global memory. Each \gls{sm} contains multiple schedulers and caches, where the schedulers actually issue the instructions to perform the execution. However the \gls{simt} execution model is used, so each instruction is executed for a group of multiple threads, referred to as a warp. If one thread within a warp needs to branch, then every thread must branch, and any that aren't required to follow this code path remain idle until the branch returns (see Figure \ref{fig:SIMT}). This general architecture is the same for all \glspl{gpu}. The remainder of this section  provides specific architecture details of NVIDIA \glspl{gpu} for completeness.}\\
      
    \begin{wrapfigure}{O}{0.5\columnwidth}%
    \begin{centering}
      \includegraphics[width=0.5\columnwidth,keepaspectratio]{\string"../resources/kepler-memory\string".pdf}
    \par\end{centering}
    \protect\caption[Illustration of the memory hierarchy present within the Kepler \gls{gpu} architecture.]{\label{fig:kepler-memory}Illustration of the memory hierarchy present within the Kepler \gls{gpu} architecture. The newer Maxwell architecture has moved the L1 cache so that it is shared with the read-only cache.}
  \end{wrapfigure}%

      Each NVIDIA \gls{gpu} consists of 1-25 \glspl{sm} and several GBs of global memory (DRAM). There are also several (significantly smaller KB scale) caches within each \gls{sm}. These caches are used for: accessing read-only texture memory; reading global memory; storing data within constant, local and shared memory. 
      
      When using \gls{cuda}, a function to be executed on the device is called a kernel. When a kernel is launched, the programmer must specify how many threads to launch, which controls how many instances of the code within the kernel are executed. The number of threads is defined by providing grid and block dimensions. Grids and blocks are both 1-3 dimensional structures, whereby each index within a grid is a block and each index within a block is a thread. At runtime blocks are assigned to \glspl{sm}, and are then further partitioned into warps of 32 threads that simultaneously execute. If less than 32 threads are required the resources for 32 threads are still used but the excess threads remain null. Blocks must be capable of executing independently, as they may execute in any order across multiple \glspl{sm}.
      
      Since the introduction of the Kepler architecture (2012) \cite{NV_KEPLER} each \gls{sm} has consisted of four schedulers capable of issuing dual instructions. This feature allows a scheduler to issue two consecutive instructions to a warp simultaneously if the instructions don't interact. Dual instructions cannot be dispatched for two different warps simultaneously. Each scheduler is capable of managing several warps (from different kernels) concurrently and instructions can be dispatched for a queued warp while the previous warp waits. This allows latencies to be hidden if enough warps are managed concurrently. The number of consecutive warps that can be managed by each scheduler is limited by the available resources required by each warp (e.g. registers or shared memory). There is also a device specific hardware limitation of the maximum threads per \gls{sm}, current devices have this limit at 2048, which divides (32 threads and 4 schedulers) to 16 warps per scheduler \cite{PM13}.
      
      Each executing thread is allocated up to 255 32-bit registers, however each SM only has 64,000 registers available \cite{MAXWELL_TUNING}. Therefore as the number of warps each SM manages increases, the available registers per thread decreases. If a thread requires more local memory than is available within the registers, memory accesses will spill from the registers into the L1 cache, increasing the latency of memory accesses. The size of the L1 cache varies between architecture and configuration from 16-48KB. If the L1 cache is filled, memory accesses subsequently spill into the L2 cache further increasing the latency of memory accesses. The L2 cache is shared by all \glspl{sm} and has a size from 256-2048KB. Furthermore, should the L2 cache become filled, memory accesses will be deferred to the global memory. Global memory has a latency 100x that of the L1 cache, therefore large quantities of register spilling can become a significant overhead.
      
      Whilst a thread is executing it also has access to global and texture memory. Global memory accesses are usually cached inside the L2 cache. When accessing large amounts of read-only data, utilisation of the read-only (texture) cache can provide faster memory reads. Devices prior to \gls{cuda} compute 3.5 are required to manually copy data to texture memory to make use of this cache, 3.5 and later devices will automatically make use of this feature where the compiler detects its suitability. Use of \lstinline[language={C++}]!const! and \lstinline[language={C++}]!__restrict__! qualifiers will assist in compiler detection, and \lstinline[language={C++}]!__ldg()! can be used to force data load via the read-only cache.
      
      There are two additional memories available to threads, constant and shared memory, however they both require special consideration. The constant cache can only process one request at a time. Memory accesses to the same address are combined, therefore it's effective throughput is divided by the number of separate requests. The constant cache is 64KB in size, and cache misses must be loaded from device memory.     
      
      Shared memory on the other-hand is \gls{on-chip} memory that is shared between all threads within a warp. This memory has higher bandwidth and lower latency than both local and global memory. Shared memory is however divided into equally sized memory banks. Memory banks are organised such that consecutive 32-bit words are assigned to separate banks. Each memory bank can only service a single 32-bit word per request, therefore if multiple threads attempt to access different addresses within the same bank, accesses to each address will be serialised. Each \gls{sm} has access to 16-64KB of shared memory, depending on architecture and configuration.
      
      Each \gls{sm} also contains a mixture of \gls{fp32} units (often referred to as \gls{cuda} cores or stream processors) and \gls{fp64} units. These units are capable of performing 32bit and 64bit arithmetic respectively. \glspl{sfu} are able to perform `fast math' operations on \gls{fp32} values.\footnote{CUDA provides several low precision intrinsic functions such as \lstinline!__sinf()! and \lstinline!__logf()! that execute in significantly less instructions than their more precise and versatile counterparts.} The quantities of these three units varies between architecture, chip and product line. For example the workstation and \gls{hpc}product lines (Quadro and Tesla respectively) generally contain \gls{ecc} memory and have a higher ratio of \gls{fp64} units to \gls{fp32} units than the domestic counterpart (GeForce). These quantities affect the operations that each device excels at.
%Could include speculative comment about fp32 pairs able to perform fp64 calculations, citing fermi whitepaper/uBench. 

      Additionally as the price for a \gls{gpu} increases, the memory bus width usually follows. The memory bus width refers to the number of bits of global memory that are read per transaction. Therefore, if data requested by a thread is a contiguous block of 128 bits, it will be read in one transaction by a memory bus of 128 bits width, rather than the multiple transactions required if the data were scattered or the memory bus smaller. Use of read-only caches can reduce the influence of this on scattered reads, unnecessary data is cached when read, and later instructions that request cached addresses then don't incur the latency of a global memory read transaction.
      
    \subsubsection{Optimisation}
      In addition to obvious considerations of the architecture introduced in the prior subsection (\ref{sub:NVIDIA-GPU-Architecture}), there are further techniques used to improve the performance of \gls{gpgpu} applications. With the primary application of \gls{gpgpu} being \gls{hpc}, optimisation guidance is far more prevalent than for \gls{cpu} optimisation, although in many cases \gls{gpu} optimisation techniques are relevant to \gls{cpu} optimisation.
      
      Traditionally in \gls{cuda}, occupancy refers to the number of active warps divided by the maximum number of warps. Ensuring maximal device occupation, so that the hardware within the architecture is utilised as much as theoretically possible, is a primary target of optimisation. This can also be referred to as thread-level parallelism. Occupancy is bounded by the limit of 2048 per \gls{sm} and the capacity of registers and caches. Providing \glspl{sm} with multiple warps or instructions to execute simultaneously helps hide memory latencies, as other instructions can be executed whilst waiting for memory.
      
      Optimal grid and block dimensions can increase occupancy via thread-level parallelism and reduce execution times in some scenarios by 23\%\ \cite{TGD13}. Whilst it is theoretically possible to calculate the best grid and block dimensions for a kernel when applied to a specific device, in most cases that can be avoided. It is often far simpler to simply benchmark the kernel against a large variety of dimensions to find the the best performing combination, which better accounts for missed variables that may affect occupancy. When purely trying to increase occupancy, register spilling is quite a realistic risk --- typically occupancy above 50\% does not translate to increased speed-up. CUDA 6.5 introduced functionality for automatically optimising block dimensions for occupancy \cite{NV_OCCUPANCY}, however these are not a complete solution to optimisation as they do not account for kernels instead bounded by memory accesses. Using the \gls{cuda} compiler (\gls{nvcc}) option \lstinline!-Xptxas -v, -abi=no! will print the number of bytes of local memory used by each kernel, providing better information for manually partitioning kernels. The \gls{cuda} profiler also contains several counters that can aide in identifying register spilling \cite{PM11}.
      
\begin{wrapfigure}{L}{0.5\textwidth}
\begin{minipage}{0.5\textwidth}
\begin{algorithm}[H]
\begin{lstlisting}
addition_kernel(a,b,c)
    i = thread.id
    c[i] = a[i] + b[i]
\end{lstlisting}
\protect\caption{\label{fig:instruction-level-parallelism-psuedo-1}Pseudo-code for a vector addition kernel without instruction level parallelism.}
\end{algorithm}
\begin{algorithm}[H]
\begin{lstlisting}
addition_kernel_ilp_x2(a,b,c)
    //Read
    i = thread.id
    ai = a[i]
    bi = b[i]
    j = i + (grid.width * block.width)
    aj = a[j]
    bj = b[j]
    //Compute
    ci = ai + bi
    cj = aj + bj
    //Write
    c[i] = ci
    c[j] = cj
\end{lstlisting}
\protect\caption{\label{fig:instruction-level-parallelism-psuedo}Pseudo-code for a vector addition kernel that has been optimised with x2 instruction level parallelism. }
\end{algorithm}
\end{minipage}
\end{wrapfigure}  
      Inversely instruction-level parallelism provides an alternate approach for optimising memory access bounded algorithms --- increasing performance whilst reducing shared memory traffic and the number of active warps. Instruction-level parallelism seeks to decrease occupancy by performing enough work within each thread to hide memory latencies without \glspl{sm} switching active warps. Additionally the increased utilisation of registers instead of shared memory allows the avoidance of bank conflicts.
      
      In essence instructions are separated and ordered within a kernel, so that operations are not performed until required data is in local memory, this allows the device to avoid waiting for memory returns or the minor overhead of rotating between warps. The pseudo-code in Algorithms \ref{fig:instruction-level-parallelism-psuedo-1} and \ref{fig:instruction-level-parallelism-psuedo} provides an example of how x2 instruction-level parallelism may be performed, although in practise using x4 or x8 (or even as high as register capacity permits) is likely to provide even greater results, due to the number of clock cycles required to perform accesses. In an ideal scenario instruction-level parallelism  was found able to achieve 87\% memory peak, when copying 56 floats per thread, this was higher than the 71\% achieved by the core \gls{cuda} function \lstinline!cudaMemcpy!. \cite{VV10} 
      
      As \glspl{gpu} follow the \gls{simt} execution model, the hardware is optimised towards static branch prediction whereby each thread within a warp should perform the same operation (\glspl{cpu} have more advanced branch prediction). To aide branch prediction the structure of code can be simplified by unrolling loops so that multiple iterations are combined into a single iteration. Unrolling a loop can reduce branching penalties, allowing improvements to performance. Volkov identified that a mutated form of loop unrolling can be used to additionally provide the benefits of instruction level parallelism \cite{VV11}.
      
      \glsadd{memory_coalescing}
      As referred to in subsection \ref{sub:NVIDIA-GPU-Architecture} the memory bus width is the number of contiguous bits accessed per read/write operation to DRAM. For this reason it becomes necessary to layout large collections of data as a \gls{soa} rather than a \gls{aos}. This layout ensures that neighbouring threads, accessing the same member of a struct specific to the thread, are accessing contiguously stored data. Reading/writing data in this format, is considered a coalesced read/write operation and the resulting global memory bandwidth occupancy is maximal. \gls{cuda} compute 3.0 and higher devices are slightly less affected by this optimisation, due to more advanced striding, however it is still a worthwhile technique due to it having similar relevance to \gls{cpu} optimisation.
      
    \subsubsection{Frameworks}
      There exist two distinct frameworks for \gls{gpgpu} application development, \gls{opencl} and \gls{cuda}. In terms of performance, both frameworks are capable of achieving similar benchmarks once tuned to the targeted device, as the code produced by each framework is very similar at a device level before compiler optimisations \cite{KDH11, FVS11}. Additionally OpenGL and DirectX both provide functionality within their graphics \glspl{api} for compute shaders. However there isn't much evidence of widespread use of compute shaders past 2010, likely due to the rise of the dedicated frameworks \gls{opencl} and \gls{cuda}. An optimised \gls{opengl} (compute shader) scan operation was found to execute 7 times slower than an optimised \gls{cuda} implementation, this difference was attributed to \gls{cuda} compiler optimisations not available to compute shaders \cite{HSO07}. Compute shaders may receive a resurgence with the release of Vulkan, as this aims to introduce lower-level controls into the graphics \glspl{api}, benefiting \gls{gpgpu} by allowing better integration with graphical outputs.
      
      \gls{opencl} is an open standard maintained by the Khronos Group, the same consortium that manages the \gls{opengl} standard, capable of targeting any parallel architecture that has appropriate vendor provided drivers. At this point in time \gls{opencl} supports: both AMD and NVIDIA \glspl{gpu}; Intel \glspl{cpu} and integrated \glspl{gpu}; several IBM products. 
        
      \gls{cuda} by comparison is a proprietary offering, restricted to NVIDIA devices. \glspl{cuda} primary advantage has been in the greater level of libraries to assist computation. It is also easier to optimise between devices, due to the architecture uniformity among NVIDIA devices, whereas \gls{opencl} supports a diverse array of hardware architectures, each requiring different optimisation priorities.
      
      Alternatively \gls{openacc} is a compiler directive standard, for accelerating serial code through the use of \gls{gpgpu}\ \cite{OPENACC}. Similar to OpenMP programmers can label their code with simple compiler directives which are then processed at compile time to include the necessary \gls{gpgpu} code. Directive standards provide a simple technique for accelerating existing code, however directives used incorrectly can provide suboptimal and potentially slower results.
      
    \subsubsection{Case Study}
      Gray, Killian et al \cite{GK*13} used \gls{gpu} processing to accelerate four computational finance applications (Black Scholes, Monte-Carlo, Bonds \& Repo) from the library QuantLib. They compared applications speed when: executed serially; executed in parallel on an 8-core \gls{cpu}; executed on a K20 \gls{gpu} when rewritten with \gls{cuda} and \gls{opencl}; executed from generated \gls{gpu} code from the directive based accelerators HMPP and OpenACC. Their results showed that the \gls{gpu} programs executed at over 100 times that of the sequential \gls{cpu} and 10 times the parallel \gls{cpu} code for the Black Scholes algorithm. The speeds of the \gls{gpu} code varied between sources, although there is nothing to suggest each could not be rewritten to achieve similar speed. For the Monte-Carlo program they found, for programs actioning over 5000 samples there was again a speed-up of at least 75 times sequential \gls{cpu} code and 10 times parallel \gls{cpu} code. This increased to 1000 times speed-up against sequential \gls{cpu} code when compared processing 50,000 samples. Once 500,000 samples was reached, however the speed suddenly dropped off. Profiling identified the cause of this was due to cache misses from the random number generation. Restructuring code to make better use of the cache would likely be capable of alleviating this fall off. The bonds application only managed an 80 times speed-up compared with sequential \gls{cpu} code, which is likely due to the amount of divergent branching within the algorithm.
      
  \subsection{Cluster, Grid \& Distributed}
    A computer cluster is a local collection of connected computers that work together as a single system. Clusters are often deployed as they can provide improved performance, availability and cost over a singular machine of similar specification. A beowulf cluster specifically refers to a cluster assembled from consumer grade computers.

    Grid and distributed computing are the techniques where large groups of computers at different locations (potentially including clusters) work in parallel on the same computational projects. In the case of grid computing, the groups of computers are often owned by various research groups, who each share access with others. In the case of distributed computing, public engagement is often leveraged, encouraging people to install the necessary software on their computers (and even phones), so that idle processing power can benefit research in fields such as medical simulations (Folding \& Home) and \gls{seti}\ \cite{And04}. Distributed computing marketing often neglects to inform users that use of their `spare' processing power will increase power usage.

    When using computer clusters, it is often the case that due to the cost of the architecture, such a cluster is shared between several research departments or universities. Similar to grid computing this is likely to lead to a schedule system, whereby jobs must be queued, removing potential for real-time applications.

  \subsection{Many Integrated Cores}
    Similar to \gls{gpu} compute, Intel now offers \gls{mic} architecture supercomputing cards (Xeon Phi), which are a direct competitor to \gls{gpgpu}, consisting of up to 72 cores capable of executing the x86 instruction set. The potential performance difference between \gls{mic} and \gls{gpu} is negligible, with each performing best in the algorithms their architectures best suit, with \gls{gpu} providing double the bandwidth of \gls{mic} for random data access \cite{TK*13}.
    
  \subsection{Summary}
    This section has shown that among the \gls{hpc} offerings, \glspl{gpu} provide an accessible and alternative to traditional \gls{hpc} systems, capable of providing up to 10x speed-up vs parallel \gls{cpu} algorithms. Optimisation is important when developing \gls{hpc} applications, of which there are many optimisation techniques and architecture features to consider when writing algorithms for \glspl{gpu}. These techniques range from interlacing instructions to ensuring memory is accessed appropriately, helping maximise utilisation of the available hardware.