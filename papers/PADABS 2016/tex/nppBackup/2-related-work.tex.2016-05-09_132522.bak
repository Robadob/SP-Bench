\section{Related Research\label{sec:related-work}}
    Neighbourhood search is most often found within agent-based models as part of \gls{sph}, collision detection within graphics often relies on similar algorithms. It is the process whereby entities within range of a location (the neighbourhood) are made accessible to the calling algorithm. Whilst various spatial data-structures such as kd-trees and R-trees are capable of providing efficient access to spatial neighbourhoods, their expensive constructions however make them unsuitable for the large dynamic agent populations found within agent-based models.
    
    The naive technique for carrying out a neighbourhood search is via a brute-force technique, individually considering whether each entity is located within the target neighbourhood. This technique may be suitable for small agent populations, however the overhead quickly becomes significant as agent populations increase, reducing the proportional size of the neighbourhoods.
    
    The most common technique now used is that of uniform spatial partitioning, whereby the environment is partitioned into a uniform grid. Entities are then sorted according to the ID of their containing cell within the grid. This allows the Moore neighbourhood of an entities cell to be accessed, ignoring entities within cells outside of the desired neighbourhood. This method is particularly suitable for parallel implementations\cite{Gre10} and several advances have been suggested to further improve the performance\cite{GS*10,Hoe14,HY*15}.
    
    Collision detection whilst also often implemented on \glspl{gpu} by contrast utilise dynamic data-structures, such as the dynamic bounding volume hierarchy \cite{LA06}. The reason for this difference is likely due to collision calculations requiring a higher surface precision than agent models which are often able to abstract entity locations to points.
    %No reason to have distinct sections
    Recent advances to fixed radius nearest neighbours have either provided no comparative performance results, or simply compared with an iteration lacking the published innovation\cite{GS*10,Hoe14,HY*15}. With numerous potential innovations which may interact and overlap it becomes necessary to standardise the methodology by which these advances can be compared both independently and in combination. When assessing the performance of \gls{hpc} algorithms there are various approaches which must be taken and considered to ensure fair results.
    
    When comparing algorithm performance across different architectures it is important to ensure that appropriate optimisations for each architecture have been carried out and that the hardware used is of a comparable grade. Historically there have been numerous cases whereby comparisons between \gls{cpu} and \gls{gpu} have shown speedups as high as 100x which have later been debunked due to flawed methodology\cite{LK*10}.
    
    However cross-architectural comparisons are not the only means by which bias can be introduced to performance results. There are a plethora of recommendations to be followed to ensure that results are not misleading\cite{Bai92}. The general trend among these guidelines is to require explicit detail of experimental conditions, ensuring uniformity between test cases as far as feasible such that results can be repeated.
      
    Microbenchmarking is also found within the \gls{hpc} community. High precision timings are collected of the repeated execution of a single operation, exposing execution costs of individual instructions and cache accesses. This work has been carried out surveying \glspl{gpu} by both Wong et al\cite{WP*10}; and Volkov and Demel\cite{VD08}, similarly Liu et al have used microbenchmarking to compare the performance within compute clusters\cite{LC*04}. Microbenchmarking primarily provides a greater understanding of architectural timings, however the lessons learned can be applied when designing \gls{hpc} algorithms. This does however make microbenchmarking unsuitable for comparing the implementations presented within this paper.