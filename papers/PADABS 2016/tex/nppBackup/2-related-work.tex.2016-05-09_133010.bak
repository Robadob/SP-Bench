\section{Related Research\label{sec:related-work}}
  FRNNs searches are most often found within agent-based models, they have also been used alongside similar algorithms within the fields of \gls{sph} and collision detection. FRNNs is the process whereby each agent considers the properties of every other agent located within a radial area about their location. This searched area can be considered the agent’s neighbourhood and must be searched every timestep of a simulation to ensure agents have live information. Whilst various spatial data-structures such as kd-trees and R-trees are capable of providing efficient access to spatial neighbourhoods, their expensive constructions however, make them unsuitable for the large dynamic agent populations found within agent-based models.

  The naive technique for carrying out a neighbourhood search is via a brute-force technique, individually considering whether each agent is located within the target neighbourhood. This technique may be suitable for small agent populations, however the overhead quickly becomes significant as agent populations increase, reducing the proportional size of the neighbourhoods.
  \begin{wrapfigure}
  
  \end{wrapfigure}
  
  The most common technique that is used to reduce the overhead of FRNNs
handling is that of uniform spatial partitioning (Figure 1), whereby the
environment is partitioned into a uniform grid. Agents are then (sor-
ted and) stored according to the ID of their containing cell within the
grid. A separate index is then produced, providing fast access the stor-
age of each cell’s agents. This allows the Moore neighbourhood of an
agent’s cell to be accessed, ignoring agents within cells outside of the
desired neighbourhood. This method is particularly suitable for parallel
implementations\cite{Gre10} and several advances have been suggested to further
improve the performance\cite{GS*10,Hoe14,HY*15}.

Recent FRNNs publications have
either provided no comparative per-
formance results, or simply compared
with an iteration lacking the pub-
lished innovation[2,4,8]. With numer-
ous potential innovations which may
interact and overlap it becomes necessary to standardise the methodology by
which these advances can be compared both independently and in combination.
When assessing the performance of High Performance Computation (HPC) al-
gorithms there are various approaches which must be taken and considered to
ensure fair results.
When comparing the performance of algorithms there are a plethora of re-
commendations to be followed to ensure that results are not misleading[1]. The
general trend among these guidelines is the requirement of explicit detailing of
experimental conditions, ensuring uniformity between test cases such that results
can be reproduced. Furthermore if comparing algorithm performance across dif-
ferent architectures it is important to ensure that appropriate optimisations for
each architecture have been carried out and that the cost of relevant hardware is
considered when discussing results. Historically there have been numerous cases
whereby comparisons between CPU and GPU have shown speedups as high as
100x which have later been debunked due to flawed methodology[5].

  %Microbenchmarking is also found within the \gls{hpc} community. High precision timings are collected of the repeated execution of a single operation, exposing execution costs of individual instructions and cache accesses. This work has been carried out surveying \glspl{gpu} by both Wong et al\cite{WP*10}; and Volkov and Demel\cite{VD08}, similarly Liu et al have used microbenchmarking to compare the performance within compute clusters\cite{LC*04}. Microbenchmarking primarily provides a greater understanding of architectural timings, however the lessons learned can be applied when designing \gls{hpc} algorithms. This does however make microbenchmarking unsuitable for comparing the implementations presented within this paper.